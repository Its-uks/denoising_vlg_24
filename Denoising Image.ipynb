{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarshverma/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, img_size=(128, 128)):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is not None:\n",
    "            # Convert BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Resize image\n",
    "            img = cv2.resize(img, img_size)\n",
    "            # Normalize to [0, 1]\n",
    "            img = img / 255.0\n",
    "            # Append to list\n",
    "            images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 485 noisy images.\n",
      "Loaded 485 clean images.\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder, img_size=(128, 128)):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is not None:\n",
    "            # Convert BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Resize image\n",
    "            img = cv2.resize(img, img_size)\n",
    "            # Normalize to [0, 1]\n",
    "            img = img / 255.0\n",
    "            # Append to list\n",
    "            images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Adjust the folder paths for macOS\n",
    "noisy_folder_path = '/Users/utkarshverma/vlg/low'\n",
    "clean_folder_path = '/Users/utkarshverma/vlg/high'\n",
    "img_size = (128, 128)  # Desired image size\n",
    "\n",
    "noisy_images = load_images_from_folder(noisy_folder_path, img_size)\n",
    "clean_images = load_images_from_folder(clean_folder_path, img_size)\n",
    "\n",
    "print(f\"Loaded {len(noisy_images)} noisy images.\")\n",
    "print(f\"Loaded {len(clean_images)} clean images.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_noisy, test_noisy, train_clean, test_clean = train_test_split(\n",
    "    noisy_images, clean_images, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    return tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_noisy_ds = tf.data.Dataset.from_tensor_slices(train_noisy).map(preprocess_image).batch(32)\n",
    "train_clean_ds = tf.data.Dataset.from_tensor_slices(train_clean).map(preprocess_image).batch(32)\n",
    "test_noisy_ds = tf.data.Dataset.from_tensor_slices(test_noisy).map(preprocess_image).batch(32)\n",
    "test_clean_ds = tf.data.Dataset.from_tensor_slices(test_clean).map(preprocess_image).batch(32)\n",
    "\n",
    "# Combine noisy and clean images for training\n",
    "train_ds = tf.data.Dataset.zip((train_noisy_ds, train_clean_ds))\n",
    "test_ds = tf.data.Dataset.zip((test_noisy_ds, test_clean_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denoising_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.UpSampling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.UpSampling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "model = create_denoising_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 394ms/step - loss: 0.0482 - val_loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=40, validation_data=test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Average PSNR: 14.653218603099651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/br5r3n552kx3klh4xq4rsk100000gn/T/ipykernel_46122/69829370.py:3: UserWarning: Inputs have mismatched dtype.  Setting data_range based on image_true.\n",
      "  psnr_values = [psnr(test_clean[i], predicted_images[i]) for i in range(len(test_clean))]\n"
     ]
    }
   ],
   "source": [
    "predicted_images = model.predict(test_noisy_ds)\n",
    "\n",
    "psnr_values = [psnr(test_clean[i], predicted_images[i]) for i in range(len(test_clean))]\n",
    "average_psnr = np.mean(psnr_values)\n",
    "print(f'Average PSNR: {average_psnr}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep CNN Autoencoder - Denoising Image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
